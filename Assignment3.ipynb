{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn import metrics\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import dill\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_set:\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.vocab = []\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.X_test = []\n",
    "        self.y_test = []\n",
    "        self.X_valid = []\n",
    "        self.y_valid = []\n",
    "    \n",
    "    \n",
    "    def process(self, raw, vocab):\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        raw = raw.translate(translator)\n",
    "        raw = raw.lower()\n",
    "        if(vocab == 0):\n",
    "            raw = raw.replace(\"\\t\", \" \")\n",
    "        raw = raw.replace(\"\\t0\\n\", \" \")\n",
    "        raw = raw.replace(\"\\t1\\n\", \" \")\n",
    "        raw = raw.replace(\"\\t2\\n\", \" \")\n",
    "        raw = raw.replace(\"\\t3\\n\", \" \")\n",
    "        raw = raw.replace(\"\\t4\\n\", \" \")\n",
    "        raw = raw.replace(\"\\t5\\n\", \" \")\n",
    "        return raw\n",
    "    \n",
    "    def load(self, filename, dset, train):\n",
    "            X = []\n",
    "            y = []\n",
    "            if(train == 1):\n",
    "                self.vocab.append(self.process(open(filename).read(), 1))\n",
    "            if(dset == 'yelp'):\n",
    "                lines = open(filename).read().splitlines()\n",
    "            if(dset == 'imdb'):\n",
    "                lines = re.split(\"(?<=[0-1])[\\n]*\", open(filename).read())\n",
    "            for x in lines:\n",
    "                X.append(x[:-1])\n",
    "                y.append(x[-1:])\n",
    "            if(dset == 'imdb'):\n",
    "                X = X[:-1]\n",
    "                y = y[:-1]\n",
    "            X_p = []\n",
    "            for i in range(len(X)):\n",
    "                X_p.append(self.process(X[i], 0))\n",
    "            return X_p, y\n",
    "    \n",
    "    def imp(self, filetrain, filevalid, filetest, dset):\n",
    "        self.X_train, self.y_train = self.load(filetrain, dset, 1)\n",
    "        self.X_valid, self.y_valid = self.load(filevalid, dset, 0)\n",
    "        self.X_test, self.y_test = self.load(filetest, dset, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "yelp = Data_set()\n",
    "yelp.imp('hwk3_datasets/yelp-train.txt', 'hwk3_datasets/yelp-valid.txt', 'hwk3_datasets/yelp-test.txt', 'yelp')\n",
    "imdb = Data_set()\n",
    "imdb.imp('hwk3_datasets/IMDB-train.txt', 'hwk3_datasets/IMDB-valid.txt', 'hwk3_datasets/IMDB-test.txt', 'imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 10000)\n",
    "vector_train = vectorizer.fit_transform(yelp.X_train)\n",
    "vector_valid = vectorizer.transform(yelp.X_valid)\n",
    "vector_test = vectorizer.transform(yelp.X_test)\n",
    "vector = vectorizer.transform(yelp.vocab)\n",
    "\n",
    "freq = vector.toarray()\n",
    "words_freq = [(word, idx, freq[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[2], reverse=True)\n",
    "#words_freq = words_freq[0:10000]\n",
    "#with open('yelp-vocab.txt', 'w') as fp: \n",
    "#    fp.write('\\n'.join('%s\\t%s\\t%s' % x for x in words_freq))\n",
    "#words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "vector = vectorizer.fit_transform(yelp.X_train)\n",
    "array = vector.toarray()\n",
    "for j in range(len(yelp.X_train)):\n",
    "    words.append([])\n",
    "    for i in range(10000):\n",
    "        if(array[j][i] != 0):\n",
    "            words[j].append([i, array[j][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "vector = vectorizer.transform(yelp.X_test)\n",
    "array = vector.toarray()\n",
    "for j in range(len(yelp.X_test)):\n",
    "    words.append([])\n",
    "    for i in range(10000):\n",
    "        if(array[j][i] != 0):\n",
    "            words[j].append([i, array[j][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"yelp-test.txt\",\"w+\")\n",
    "#for i in range(len(words)):\n",
    "#    for y in words[i]:\n",
    "#        f.write('%s\\t' % str(y[0]))\n",
    "#    f.write(yelp.y_test[i])\n",
    "#    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = yelp.y_train\n",
    "y_valid = yelp.y_valid\n",
    "y_test = yelp.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbow = np.zeros(shape=(7000,10000), dtype=np.int64)\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words[i])):\n",
    "            bbow[i][words[i][j][0]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2005\n",
      "0.351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='uniform').fit(bbow, y_train)\n",
    "y_pr = clf.predict(vector_test)\n",
    "clf = DummyClassifier(strategy='most_frequent').fit(bbow, y_train)\n",
    "y_pred = clf.predict(vector_test.toarray())\n",
    "print(metrics.f1_score(y_test, y_pr, average='micro'))\n",
    "print(metrics.f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'alpha': [0.01, 1, 0.5, 0.2, 0.1]}],\n",
       "       pre_dispatch='2*n_jobs', refit='f1_micro',\n",
       "       return_train_score='warn', scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB \n",
    "tuned_parameters = [{'alpha': [0.01, 1, 0.5, 0.2, 0.1]}]\n",
    "n_folds = 5\n",
    "clf = BernoulliNB()\n",
    "clf = GridSearchCV(clf, tuned_parameters, scoring='f1_micro', cv=n_folds, refit='f1_micro')\n",
    "clf.fit(bbow, y_train)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "0.7482857142857143\n",
      "0.425\n",
      "0.437\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_estimator_)\n",
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(metrics.f1_score(y_train, y_pr, average='micro'))\n",
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(metrics.f1_score(y_valid, y_pr, average='micro'))\n",
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'max_depth': [1, 2, 10, 32], 'min_samples_split': [0.1, 0.3, 0.7, 1.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit='f1_micro',\n",
       "       return_train_score='warn', scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tuned_parameters = [{'max_depth': [1, 2, 10, 32], 'min_samples_split': [0.1, 0.3, 0.7, 1.0]}]\n",
    "n_folds = 3\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(clf, tuned_parameters, scoring='f1_micro', cv=n_folds, refit='f1_micro')\n",
    "clf.fit(bbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=0.1,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "0.43257142857142855\n",
      "0.38499999999999995\n",
      "0.405\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_estimator_)\n",
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(metrics.f1_score(y_train, y_pr, average='micro'))\n",
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(metrics.f1_score(y_valid, y_pr, average='micro'))\n",
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=5000,\n",
      "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "     verbose=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "tuned_parameters = [{'C': [0.001, 0.01, 0.1, 1, 10]}]\n",
    "n_folds = 3\n",
    "clf = LinearSVC(random_state=0, max_iter=5000)\n",
    "clf = GridSearchCV(clf, tuned_parameters, scoring='f1_micro', cv=n_folds, refit='f1_micro')\n",
    "clf.fit(bbow, y_train)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM BBoW train 0.5515714285714286\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(\"Linear SVM BBoW train\", metrics.f1_score(y_train, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM BBoW valid 0.452\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(\"Linear SVM BBoW valid\", metrics.f1_score(y_valid, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM BBoW test 0.4755\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(\"Linear SVM BBoW test\", metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbow = np.zeros(shape=(7000,10000))\n",
    "array = vector_train.toarray()\n",
    "for i in range(len(words)):\n",
    "    sum_w = np.sum(words[i], axis=0)\n",
    "    for j in range(len(words[i])):\n",
    "            fbow[i][words[i][j][0]] = array[i][words[i][j][0]] / sum_w[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.189\n",
      "0.351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='uniform', random_state=0).fit(fbow, y_train)\n",
    "y_pr = clf.predict(vector_test)\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0).fit(fbow, y_train)\n",
    "y_pred = clf.predict(vector_test)\n",
    "print(metrics.f1_score(y_test, y_pr, average='micro'))\n",
    "print(metrics.f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9385714285714286\n",
      "0.356\n",
      "0.3495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB().fit(fbow, y_train)\n",
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(metrics.f1_score(y_train, y_pr, average='micro'))\n",
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(metrics.f1_score(y_valid, y_pr, average='micro'))\n",
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'max_depth': [1, 2, 10, 32], 'min_samples_split': [0.1, 0.3, 0.7, 1.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit='f1_micro',\n",
       "       return_train_score='warn', scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tuned_parameters = [{'max_depth': [1, 2, 10, 32], 'min_samples_split': [0.1, 0.3, 0.7, 1.0]}]\n",
    "n_folds = 3\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(clf, tuned_parameters, scoring='f1_micro', cv=n_folds, refit='f1_micro')\n",
    "clf.fit(fbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=32,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=0.1,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "0.4308571428571429\n",
      "0.408\n",
      "0.4144999999999999\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_estimator_)\n",
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(metrics.f1_score(y_train, y_pr, average='micro'))\n",
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(metrics.f1_score(y_valid, y_pr, average='micro'))\n",
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=5000,\n",
      "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "     verbose=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "tuned_parameters = [{'C': [0.001, 0.01, 0.1, 1, 10]}]\n",
    "n_folds = 3\n",
    "clf = LinearSVC(random_state=0, max_iter=5000)\n",
    "clf = GridSearchCV(clf, tuned_parameters, scoring='f1_micro', cv=n_folds, refit='f1_micro')\n",
    "clf.fit(fbow, y_train)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM FBoW train 0.6435714285714286\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(\"Linear SVM FBoW train\", metrics.f1_score(y_train, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM FBoW valid 0.456\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(\"Linear SVM FBoW valid\", metrics.f1_score(y_valid, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM FBoW test 0.461\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(\"Linear SVM FBoW test\", metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector = vectorizer.transform(imdb.vocab)\n",
    "#freq = vector.toarray()\n",
    "#words_freq = [(word, idx, freq[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#words_freq = sorted(words_freq, key = lambda x: x[2], reverse=True)\n",
    "#words_freq = words_freq[0:10000]\n",
    "#with open('imdb-vocab.txt', 'w') as fp: \n",
    "#    fp.write('\\n'.join('%s\\t%s\\t%s' % x for x in words_freq))\n",
    "#words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(dset):\n",
    "    words = []\n",
    "    vector = vectorizer.transform(dset)\n",
    "    array = vector.toarray()\n",
    "    for j in range(len(dset)):\n",
    "        words.append([])\n",
    "        for i in range(10000):\n",
    "            if(array[j][i] != 0):\n",
    "                words[j].append([i, array[j][i]])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"IMDB-test.txt\",\"w+\")\n",
    "#for i in range(len(words)):\n",
    "#    for y in words[i]:\n",
    "#        f.write('%s\\t' % str(y[0]))\n",
    "#    f.write(imdb.y_test[i])\n",
    "#    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 10000)\n",
    "vector_train = vectorizer.fit_transform(imdb.X_train)\n",
    "vector_valid = vectorizer.transform(imdb.X_valid)\n",
    "vector_test = vectorizer.transform(imdb.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_imdb_train = get_words(imdb.X_train)\n",
    "words_imdb_valid = get_words(imdb.X_valid)\n",
    "words_imdb_test = get_words(imdb.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_X_comb = []\n",
    "for i in range(len(imdb.X_train)):\n",
    "    imdb_X_comb.append(imdb.X_train[i])\n",
    "for i in range(len(imdb.X_valid)):\n",
    "    imdb_X_comb.append(imdb.X_valid[i])\n",
    "words_imdb_comb = get_words(imdb_X_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbow = np.zeros(shape=(15000,10000), dtype=np.int64)\n",
    "for i in range(len(words_imdb_train)):\n",
    "    for j in range(len(words_imdb_train[i])):\n",
    "            bbow[i][words_imdb_train[i][j][0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbow = np.zeros(shape=(15000,10000))\n",
    "array = vector_train.toarray()\n",
    "for i in range(len(words_imdb_train)):\n",
    "    sum_w = np.sum(words_imdb_train[i], axis=0)\n",
    "    for j in range(len(words_imdb_train[i])):\n",
    "            fbow[i][words_imdb_train[i][j][0]] = array[i][words_imdb_train[i][j][0]] / sum_w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_y_comb = []\n",
    "for i in range(len(imdb.y_train)):\n",
    "    imdb_y_comb.append(imdb.y_train[i])\n",
    "for i in range(len(imdb.y_valid)):\n",
    "    imdb_y_comb.append(imdb.y_valid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = imdb.y_train\n",
    "y_valid = imdb.y_valid\n",
    "y_test = imdb.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform BBoW:  0.49972\n",
      "Majority BBoW:  0.5\n",
      "Uniform FBoW:  0.49508\n",
      "Majority FBoW:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='uniform').fit(bbow, y_train)\n",
    "y_pr = clf.predict(vector_test)\n",
    "print(\"Uniform BBoW: \", metrics.f1_score(y_test, y_pr, average='micro'))\n",
    "clf = DummyClassifier(strategy='uniform').fit(fbow, y_train)\n",
    "y_pr = clf.predict(vector_test)\n",
    "print(\"Uniform FBoW: \", metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB BBoW train:  0.8729999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = BernoulliNB(alpha=0.2).fit(bbow, y_train)\n",
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(\"BernoulliNB BBoW train: \", metrics.f1_score(y_train, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB BBoW valid:  0.8425\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(\"BernoulliNB BBoW valid: \", metrics.f1_score(y_valid, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB BBoW test:  0.8354\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(\"BernoulliNB BBoW test: \", metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB FBoW train:  0.8744666666666666\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB().fit(fbow, y_train)\n",
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(\"GaussianNB FBoW train: \", metrics.f1_score(y_train, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB FBoW valid:  0.7753\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(\"GaussianNB FBoW valid: \", metrics.f1_score(y_valid, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB FBoW test:  0.72016\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(\"GaussianNB FBoW test: \", metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'max_depth': [1, 2, 10, 32], 'min_samples_split': [0.1, 0.3, 0.7, 1.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit='f1_micro',\n",
       "       return_train_score='warn', scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_parameters = [{'max_depth': [1, 2, 10, 32], 'min_samples_split': [0.1, 0.3, 0.7, 1.0]}]\n",
    "n_folds = 3\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(clf, tuned_parameters, scoring='f1_micro', cv=n_folds, refit='f1_micro')\n",
    "clf.fit(bbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=32,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=0.1,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree BBoW train:  0.7412666666666666\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(\"Decision Tree BBoW train: \", metrics.f1_score(y_train, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree BBoW valid:  0.7249\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(\"Decision Tree BBoW valid: \", metrics.f1_score(y_valid, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree BBoW test:  0.72824\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(\"Decision Tree BBoW test: \", metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'max_depth': [1, 2, 10, 32], 'min_samples_split': [0.1, 0.3, 0.7, 1.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit='f1_micro',\n",
       "       return_train_score='warn', scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_parameters = [{'max_depth': [1, 2, 10, 32], 'min_samples_split': [0.1, 0.3, 0.7, 1.0]}]\n",
    "n_folds = 3\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(clf, tuned_parameters, scoring='f1_micro', cv=n_folds, refit='f1_micro')\n",
    "clf.fit(fbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=32,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=0.1,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree BBoW train:  0.7203333333333334\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(\"Decision Tree BBoW train: \", metrics.f1_score(y_train, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree BBoW valid:  0.7172\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(\"Decision Tree BBoW valid: \", metrics.f1_score(y_valid, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree BBoW test:  0.7148\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(\"Decision Tree BBoW test: \", metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'C': [0.001, 0.01, 0.1, 1, 10]}]\n",
    "n_folds = 3\n",
    "clf = LinearSVC(random_state=0, max_iter=5000)\n",
    "clf = GridSearchCV(clf, tuned_parameters, scoring='f1_micro', cv=n_folds, refit='f1_micro')\n",
    "clf.fit(bbow, y_train)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0, max_iter=5000, C=0.01).fit(bbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM BBoW train:  0.935\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(\"Linear SVM BBoW train: \", metrics.f1_score(y_train, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM BBoW valid:  0.8668\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(\"Linear SVM BBoW valid: \", metrics.f1_score(y_valid, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM BBoW test:  0.85844\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(\"Linear SVM BBoW test: \", metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=50, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=5000,\n",
      "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "     verbose=0)\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'C': [10, 20, 50]}]\n",
    "n_folds = 3\n",
    "clf = LinearSVC(random_state=0, max_iter=5000)\n",
    "clf = GridSearchCV(clf, tuned_parameters, scoring='f1_micro', cv=n_folds, refit='f1_micro')\n",
    "clf.fit(fbow, y_train)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0, max_iter=5000, C=50).fit(fbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM FBoW train:  0.8496\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_train.toarray())\n",
    "print(\"Linear SVM FBoW train: \", metrics.f1_score(y_train, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM FBoW valid:  0.8078\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_valid.toarray())\n",
    "print(\"Linear SVM FBoW valid: \", metrics.f1_score(y_valid, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM FBoW test:  0.8056799999999998\n"
     ]
    }
   ],
   "source": [
    "y_pr = clf.predict(vector_test.toarray())\n",
    "print(\"Linear SVM FBoW test: \", metrics.f1_score(y_test, y_pr, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
